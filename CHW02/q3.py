# -*- coding: utf-8 -*-
"""Q3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/172AJozP8AfaRgzTG1nzjY6Fved52meam

<h1 align="center">Introduction to Machine Learning - 25737-2</h1>
<h4 align="center">Dr. R. Amiri</h4>
<h4 align="center">Sharif University of Technology, Spring 2024</h4>


**<font color='red'>Plagiarism is strongly prohibited!</font>**


**Student Name**: Hossein Anjidani

**Student ID**: 400100746

## Importing Libraries

First we import libraries that we need for this assignment.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import drive
import os
from sklearn.model_selection import train_test_split
# import any other libraries needed below

drive.mount('/content/drive')
path = os.path.join('/content/drive/MyDrive/UNI/Sem 6/ML/IML_CHW2/', 'Q3')

"""## Reading Data and Preprocessing

In this section, we want to read data from a CSV file and then preprocess it to make it ready for the rest of the problem.

First, we read the data in the cell below and extract an $m \times n$ matrix, $X$, and an $m \times 1$ vector, $Y$, from it, which represent our knowledge about the features of the data (`X1`, `X2`, `X3`) and the class (`Y`), respectively. Note that by $m$, we mean the number of data points and by $n$, we mean the number of features.
"""

X, Y = None, None

### START CODE HERE ###
df = pd.read_csv(os.path.join(path, 'data2_logistic.csv'))
# print(df)
X = df.drop('Y', axis=1)
Y = df['Y']
### END CODE HERE ###

print(X.shape)
print(Y.shape)

"""Next, we should normalize our data. For normalizing a vector $\mathbf{x}$, a very common method is to use this formula:

$$
\mathbf{x}_{norm} = \dfrac{\mathbf{x} - \overline{\mathbf{x}}}{\sigma_\mathbf{x}}
$$

Here, $\overline{x}$ and $\sigma_\mathbf{x}$ denote the mean and standard deviation of vector $\mathbf{x}$, respectively. Use this formula and store the new $X$ and $Y$ vectors in the cell below.

**Question**: Briefly explain why we need to normalize our data before starting the training.

**Answer**:
1. Improved Model Performance: Normalization helps to ensure that all features are on a similar scale, which can improve the performance of many machine learning models. This is especially important for models that rely on distance-based computations, such as k-nearest neighbors (KNN) or support vector machines (SVMs), where features with larger ranges can dominate the distance metric.
2. Faster Convergence: Normalization can also help speed up the convergence of gradient-based optimization algorithms, such as those used in neural networks. When the features have vastly different scales, the optimization process can become inefficient, as the gradients for the different features will have very different magnitudes. Normalizing the features helps to ensure that the gradients are on a similar scale, leading to faster convergence.
3. Avoiding Numerical Issues: Unnormalized data can sometimes lead to numerical issues, such as overflow or underflow, when used as input to certain machine learning models. Normalization helps to avoid these problems by ensuring that the data is within a reasonable range.
4. Preventing Data Leakage: If you normalize the data after splitting it into training and test sets, you can inadvertently introduce data leakage, where information from the test set is used to influence the training process. This can lead to overly optimistic performance estimates. Normalizing the data before the split ensures that the test set remains completely independent.
"""

### START CODE HERE ###
X = (X - X.mean(axis=0)) / X.std(axis=0)
### END CODE HERE ###

"""Finally, we should add a column of $1$s at the beginning of $X$ to represent the bias term. Do this in the next cell. Note that after this process, $X$ should be an $m \times (n+1)$ matrix."""

### START CODE HERE ###
X = np.hstack((np.ones((X.shape[0], 1)), X))
### END CODE HERE ###

print(X.shape)

"""## Training Model

### Sigmoid Function
You should begin by implementing the $\sigma(\mathbf{x})$ function. Recall that the logistic regression hypothesis $\mathcal{h}()$ is defined as:
$$
\mathcal{h}_{\theta}(\mathbf{x}) = \mathcal{g}(\theta^\mathbf{T}\mathbf{x})
$$
where $\mathcal{g}()$ is the sigmoid function as:
$$
\mathcal{g}(\mathbf{z}) = \frac{1}{1+exp^{-\mathbf{z}}}
$$
The Sigmoid function has the property that $\mathbf{g}(+\infty)\approx 1$ and $\mathcal{g}(−\infty)\approx0$. Test your function by calling `sigmoid(z)` on different test samples. Be certain that your sigmoid function works with both vectors and matrices - for either a vector or a matrix, your function should perform the sigmoid function on every element.
"""

def sigmoid(Z):
    '''
    Applies the sigmoid function on every element of Z
    Arguments:
        Z can be a (n,) vector or (n , m) matrix
    Returns:
        A vector/matrix, same shape with Z, that has the sigmoid function applied elementwise
    '''
    ### START CODE HERE ###
    return 1 / (1 + np.exp(-Z))
    ### END CODE HERE ###

# Test with a single value
print(sigmoid(0))
print(sigmoid(10))
print(sigmoid(-10))

# Test with a vector
z = np.array([-5, -1, 0, 1, 5])
print(sigmoid(z))

# Test with a matrix
z = np.array([[-5, -1, 0], [1, 5, 10]])
print(sigmoid(z))

"""### Cost Function
Implement the functions to compute the cost function. Recall the cost function for logistic regression is a scalar value given by:
$$
\mathcal{J}(\theta) = \sum_{i=1}^{n}[-y^{(i)}\log{(\mathcal{h}_\theta(\mathbf{x}^{(i)}))}-(1-y^{(i)})\log{(1-\mathcal{h}_\theta(\mathbf{x}^{(i)}))}] + \frac{\lambda}{2}||\theta||_2^2
$$
"""

def computeCost(theta, X, y, regLambda):
    '''
    Computes the objective function
    Arguments:
        theta is d-dimensional numpy vector
        X is a n-by-d numpy matrix
        y is an n-dimensional numpy vector
        regLambda is the scalar regularization constant
    Returns:
        a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **
    '''

    m, n = X.shape
    loss = None
    ### START CODE HERE ###
    h = sigmoid(X @ theta)
    # Compute the unregularized cost
    loss = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    # Add L2 regularization
    if regLambda > 0:
        reg_term = (regLambda / (2 * m)) * np.sum(theta[1:]**2)
        loss += reg_term
    ### END CODE HERE ###
    return loss

"""### Gradient of the Cost Function
Now, we want to calculate the gradient of the cost function. The gradient of the cost function is a d-dimensional vector.\
We must be careful not to regularize the $\theta_0$ parameter (corresponding to the first feature we add to each instance), and so the 0's element is given by:
$$
\frac{\partial \mathcal{J}(\theta)}{\partial \theta_0} = \sum_{i=1}^n (\mathcal{h}_\theta(\mathbf{x}^{(i)})-y^{(i)})
$$

Question: What is the answer to this problem for the $j^{th}$ element (for $j=1...d$)?

Answer:
"""

def computeGradient(theta, X, y, regLambda):
    '''
    Computes the gradient of the objective function
    Arguments:
        theta is d-dimensional numpy vector
        X is a n-by-d numpy matrix
        y is an n-dimensional numpy vector
        regLambda is the scalar regularization constant
    Returns:
        the gradient, an d-dimensional vector
    '''

    m, n = X.shape
    grad = None
    ### START CODE HERE ###
    grad = np.zeros_like(theta)

    h = sigmoid(-X @ theta)  # Compute the hypothesis

    # Compute the gradient for the bias term (θ0)
    grad[0] = (1/m) * np.sum(h - y)

    # Compute the gradient for the other parameters (θj, j=1...d)
    grad[1:] = (1/m) * (X[:, 1:].T @ (h - y)) + (regLambda/m) * theta[1:]

    ### END CODE HERE ###
    return grad

"""### Training and Prediction
Once you have the cost and gradient functions complete, implemen tthe fit and predict methods.\
Your fit method should train the model via gradient descent, relying on the cost and gradient functions. This function should return two parameters. The first parameter is $\theta$, and the second parameter is a `numpy` array that contains the loss in each iteration. This array is indicated by `loss_history` in the code.\
Instead of simply running gradient descent for a specific number of iterations, we will use a more sophisticated method: we will stop it after the solution hasconverged. Stop the gradient descent procedure when $\theta$ stops changing between consecutive iterations. You can detect this convergence when:
$$
||\theta_{new}-\theta_{old}||_2 <= \epsilon,
$$
for some small $\epsilon$ (e.g, $\epsilon=10E-4$).\
For readability, we’d recommend implementing this convergence test as a dedicated function `hasConverged`.
"""

def fit(X, y, regLambda = 0.01, alpha = 0.01, epsilon = 1e-4, maxNumIters = 100):
    '''
    Trains the model
    Arguments:
        X           is a n-by-d numpy matrix
        y           is an n-dimensional numpy vector
        maxNumIters is the maximum number of gradient descent iterations
        regLambda   is the scalar regularization constant
        epsilon     is the convergence rate
        alpha       is the gradient descent learning rate
    '''

    m, n = X.shape
    theta, loss_history = None, None
    ### START CODE HERE ###
    theta = np.zeros(n)
    loss_history = []
    for i in range(maxNumIters):
      theta_old = theta.copy()
      grad =  computeGradient(theta, X, y, regLambda)
      theta -= alpha * grad
      loss_history.append(computeCost(theta, X, y, regLambda))
      if hasConverged(theta_old, theta, epsilon):
        break
    ### END CODE HERE ###
    return theta, loss_history




def hasConverged(theta_old, theta_new, epsilon):
    '''
    Return if the theta converged or not
    Arguments:
        theta_old   is the theta calculated in prevoius iteration
        theta_new   is the theta calculated in current iteration
        epsilon     is the convergence rate
    '''

    ### START CODE HERE ###
    return np.linalg.norm(theta_new - theta_old) <= epsilon
    ### END CODE HERE ###
    # return False

"""Finally, we want to evaluate our loss for this problem. Complete the cell below to calculate and print the loss of each iteration and the final theta of your model."""

theta, loss_history = fit(X, Y) # calculating theta and loss of each iteration

### START CODE HERE ###
print(theta)
print(loss_history)
plt.plot(loss_history)
### END CODE HERE ###

"""### Testing Your Implementation
To test your logistic regression implementation, first you should use `train_test_split` function to split dataset into three parts:

- 70% for the training set
- 20% for the validation set
- 10% for the test set

Do this in the cell below.
"""

X_train, Y_train, X_val, Y_val, X_test, Y_test = None, None, None, None, None, None

### START CODE HERE ###
X_train, X_temp, y_train, y_temp = train_test_split(X, Y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42)
### END CODE HERE ###

"""Then, you should complete `predict` function to find the weight vector and the loss on the test data."""

def predict(theta, X):
    '''
    Use the model to predict values for each instance in X
    Arguments:
        theta is d-dimensional numpy vector
        X     is a n-by-d numpy matrix
    Returns:
        an n-dimensional numpy vector of the predictions, the output should be binary (use h_theta > .5)
    '''

    Y = None
    ### START CODE HERE ###
    h = sigmoid(np.dot(X, theta))
    Y = (h > 0.5).astype(int)
    ### END CODE HERE ###
    return Y

"""Now, run the `fit` and `predict` function for different values of the learning rate and regularization constant. Plot the `loss_history` of these different values for train and test data both in the same figure.

**Question**: Discuss the effect of the learning rate and regularization constant and find the best values of these parameters.

**Answer**:
"""

import matplotlib.pyplot as plt

# Define a range of learning rates and regularization constants to experiment with
learning_rates = [0.001, 0.01, 0.1]
regularization_constants = [0.001, 0.01, 0.1]

# Initialize lists to store loss_history for train and test data
train_losses = []
test_losses = []

# Iterate over different values of learning rates and regularization constants
for lr in learning_rates:
    for reg_const in regularization_constants:
        # Train the model using the fit function
        theta, loss_history = fit(X_train, y_train, regLambda=reg_const, alpha=lr)

        # Predict on the test data to calculate the loss
        y_test_pred = predict(theta, X_test)
        test_loss = computeCost(theta, X_test, y_test, reg_const)

        # Store the loss_history for train data
        train_losses.append(loss_history)

        # Store the loss for test data
        test_losses.append(test_loss)

# Plot the loss_history for different values of learning rates and regularization constants
plt.figure(figsize=(12, 6))
for i in range(len(learning_rates)):
    for j in range(len(regularization_constants)):
        plt.plot(train_losses[i*len(regularization_constants) + j], label=f"LR: {learning_rates[i]}, Reg Const: {regularization_constants[j]} (Train)")
        plt.axhline(y=test_losses[i*len(regularization_constants) + j], color='r', linestyle='--', label=f"LR: {learning_rates[i]}, Reg Const: {regularization_constants[j]} (Test)")

plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Loss History for Different Learning Rates and Regularization Constants')
plt.legend()
plt.show()

"""## Naive Bayes

In this part, you will use the `GaussianNB` classifier to classify the data. You should not change the default parameters of this classifier. First, train the classifier on the training set and then find the accuracy of it on the test set.

**Question**: What is the accuracy of this method on test set?

**Answer**:
"""

from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score

# Initialize the Gaussian Naive Bayes classifier
gnb = GaussianNB()

# Train the classifier on the training set
gnb.fit(X_train, y_train)

# Predict the labels for the test set
y_pred = gnb.predict(X_test)

# Calculate the accuracy of the classifier on the test set
accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of Gaussian Naive Bayes on the test set:", accuracy)

"""## LDA (Linear Discriminant Analysis)

In this part, you will use the `LinearDiscriminantAnalysis` classifier to classify the data. You should not change the default parameters of this classifier. First, train the classifier on the training set and then find the accuracy of it on the test set.

**Question**: What is the accuracy of this method on test set?

**Answer**:
"""

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.metrics import accuracy_score

# Initialize the Linear Discriminant Analysis classifier
lda = LinearDiscriminantAnalysis()

# Train the classifier on the training set
lda.fit(X_train, y_train)

# Predict the labels for the test set
y_pred = lda.predict(X_test)

# Calculate the accuracy of the classifier on the test set
accuracy = accuracy_score(y_test, y_pred)

print("Accuracy of Linear Discriminant Analysis on the test set:", accuracy)

"""## Conclution

**Question**: What is the best method for classifying this dataset? What is the best accuracy on the test set?

**Answer**:Based on the output accuracy, LDA seems to work better.
"""